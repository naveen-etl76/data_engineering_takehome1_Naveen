{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark   # Import the findspark library, which helps locate the local Spark installation\n",
    "\n",
    "findspark.init()   # Initialize findspark so that PySpark can be imported and used in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NYC Jobs Dataset Analysis This notebook demonstrates a production-grade ETL pipeline using PySpark. We will:\n",
    "- Profile source data \n",
    "- Clean & preprocess \n",
    "- Apply feature engineering \n",
    "- Remove irrelevant features \n",
    "- Store processed data \n",
    "- Resolve KPIs \n",
    "- Visualize results \n",
    "- Run test cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"NYCJobsAnalysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = spark.read.csv(\"/dataset/nyc-jobs.csv\", header=True, sep=\",\", quote='\"', multiLine=True, escape='\"') \n",
    "df_raw.printSchema() \n",
    "df_raw.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profiling: count nulls per column \n",
    "for col_name in df_raw.columns: print(col_name, df_raw.filter(df_raw[col_name].isNull()).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions (Preprocessing + Feature Engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, year, to_timestamp, explode, split, trim, avg, max, min, count, dense_rank, row_number, current_date\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Preprocessing\n",
    "def cast_columns(df):\n",
    "    return df.withColumn(\"Salary_From\", col(\"Salary Range From\").cast(\"double\")) \\\n",
    "             .withColumn(\"Salary_To\", col(\"Salary Range To\").cast(\"double\"))\n",
    "\n",
    "def normalize_salary(df):\n",
    "    return df.withColumn(\n",
    "        \"Salary_Normalized\",\n",
    "        when(col(\"Salary Frequency\") == \"Annual\", col(\"Salary_To\"))\n",
    "        .when(col(\"Salary Frequency\") == \"Daily\", col(\"Salary_To\") * 260)\n",
    "        .when(col(\"Salary Frequency\") == \"Weekly\", col(\"Salary_To\") * 52)\n",
    "        .when(col(\"Salary Frequency\") == \"Hourly\", col(\"Salary_To\") * 2080)\n",
    "        .otherwise(None)\n",
    "    )\n",
    "\n",
    "def categorize_degree(df):\n",
    "    return df.withColumn(\n",
    "        \"Degree_Level\",\n",
    "        when(col(\"Minimum Qual Requirements\").rlike(\"baccalaureate|Bachelor\"), \"Bachelor\")\n",
    "        .when(col(\"Minimum Qual Requirements\").rlike(\"Master\"), \"Master\")\n",
    "        .when(col(\"Minimum Qual Requirements\").rlike(\"Doctor|PhD\"), \"Doctorate\")\n",
    "        .when(col(\"Minimum Qual Requirements\").rlike(\"High school|equivalent\"), \"High School\")\n",
    "        .otherwise(\"Other\")\n",
    "    )\n",
    "\n",
    "def extract_skills(df):\n",
    "    return df.withColumn(\"Skill\", explode(split(col(\"Preferred Skills\"), \"â€¢|,|;\"))) \\\n",
    "             .withColumn(\"Skill\", trim(col(\"Skill\")))\n",
    "\n",
    "def add_posting_year(df):\n",
    "    return df.withColumn(\"Posting_Year\", year(to_timestamp(col(\"Posting Date\"), \"yyyy-MM-dd'T'HH:mm:ss.SSS\")))\n",
    "\n",
    "def remove_unnecessary_features(df):\n",
    "    return df.drop(\"Recruitment Contact\",\"Additional Information\",\"To Apply\",\"Process Date\",\"Post Until\",\"Posting Updated\",\"Work Location 1\",\"Division/Work Unit\",\"Hours/Shift\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions (KPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KPI Functions\n",
    "def jobs_per_category(df, top_n=10):\n",
    "    category_counts = df.groupBy(\"Job Category\").agg(count(\"*\").alias(\"Job_Count\"))\n",
    "    windowSpec = Window.orderBy(col(\"Job_Count\").desc())\n",
    "    return (category_counts.withColumn(\"rank\", dense_rank().over(windowSpec))\n",
    "                           .filter(col(\"rank\") <= top_n)\n",
    "                           .orderBy(col(\"rank\")))\n",
    "\n",
    "def salary_distribution(df):\n",
    "    return df.groupBy(\"Job Category\").agg(\n",
    "        avg(\"Salary_Normalized\").alias(\"Avg_Salary\"),\n",
    "        min(\"Salary_Normalized\").alias(\"Min_Salary\"),\n",
    "        max(\"Salary_Normalized\").alias(\"Max_Salary\")\n",
    "    ).orderBy(col(\"Avg_Salary\").desc())\n",
    "\n",
    "def degree_salary_correlation(df):\n",
    "    return df.groupBy(\"Degree_Level\").agg(avg(\"Salary_Normalized\").alias(\"Avg_Salary\")).orderBy(col(\"Avg_Salary\").desc())\n",
    "\n",
    "def highest_salary_per_agency(df):\n",
    "    df_norm = (df.withColumn(\"Salary_To\", col(\"Salary Range To\").cast(\"double\"))\n",
    "                 .withColumn(\"Salary_Normalized\",\n",
    "                     when(col(\"Salary Frequency\") == \"Annual\", col(\"Salary_To\"))\n",
    "                     .when(col(\"Salary Frequency\") == \"Daily\", col(\"Salary_To\") * 260)\n",
    "                     .when(col(\"Salary Frequency\") == \"Weekly\", col(\"Salary_To\") * 52)\n",
    "                     .when(col(\"Salary Frequency\") == \"Hourly\", col(\"Salary_To\") * 2080)\n",
    "                     .otherwise(None))\n",
    "                 .select(\"Agency\",\"Business Title\",\"Job Category\",\"Salary Frequency\",\n",
    "                         \"Salary Range From\",\"Salary Range To\",\"Salary_Normalized\"))\n",
    "    windowSpec = Window.partitionBy(\"Agency\").orderBy(col(\"Salary_Normalized\").desc())\n",
    "    return (df_norm.withColumn(\"rank\", row_number().over(windowSpec))\n",
    "                   .filter(col(\"rank\") == 1)\n",
    "                   .select(\"Agency\",\"Business Title\",\"Job Category\",\"Salary Frequency\",\"Salary Range From\",\"Salary Range To\",\"Salary_Normalized\"))\n",
    "\n",
    "def avg_salary_last_2_years(df):\n",
    "    current_year_val = year(current_date())\n",
    "    recent_jobs = df.filter(col(\"Posting_Year\") >= (current_year_val - 2))\n",
    "    return recent_jobs.groupBy(\"Agency\").agg(avg(\"Salary_Normalized\").alias(\"Avg_Salary\")).orderBy(col(\"Avg_Salary\").desc())\n",
    "\n",
    "def highest_paid_skills(df, top_n=10):\n",
    "    return df.groupBy(\"Skill\").agg(\n",
    "        avg(\"Salary_Normalized\").alias(\"Avg_Salary\"),\n",
    "        max(\"Salary_Normalized\").alias(\"Max_Salary\")\n",
    "    ).orderBy(col(\"Avg_Salary\").desc()).limit(top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cast_columns(df_raw)\n",
    "df = normalize_salary(df)\n",
    "df = categorize_degree(df)\n",
    "df = extract_skills(df)\n",
    "df = add_posting_year(df)\n",
    "df = remove_unnecessary_features(df)\n",
    "df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet(\"/dataset/nyc-jobs-processed.parquet\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run KPIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_per_category(df).show(truncate=False)\n",
    "salary_distribution(df).show(truncate=False)\n",
    "degree_salary_correlation(df).show(truncate=False)\n",
    "highest_salary_per_agency(df).show(truncate=False)\n",
    "avg_salary_last_2_years(df).show(truncate=False)\n",
    "highest_paid_skills(df).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Top 10 job categories\n",
    "top_categories = jobs_per_category(df).toPandas()\n",
    "sns.barplot(x=\"Job_Count\", y=\"Job Category\", data=top_categories)\n",
    "plt.title(\"Top 10 Job Categories by Posting Count\")\n",
    "plt.show()\n",
    "\n",
    "# Average salary by degree level\n",
    "salary_by_degree = degree_salary_correlation(df).toPandas()\n",
    "sns.barplot(x=\"Degree_Level\", y=\"Avg_Salary\", data=salary_by_degree)\n",
    "plt.title(\"Average Salary by Degree Level\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salary normalization test\n",
    "test_df = spark.createDataFrame([(\"Hourly\", 40.0)], [\"Salary Frequency\",\"Salary_To\"])\n",
    "test_df = normalize_salary(test_df)\n",
    "assert test_df.collect()[0][\"Salary_Normalized\"] == 40*2080\n",
    "\n",
    "# Degree categorization test\n",
    "test_df = spark.createDataFrame([(\"A baccalaureate degree required\")], [\"Minimum Qual Requirements\"])\n",
    "test_df = categorize_degree(test_df)\n",
    "assert test_df.collect()[0][\"Degree_Level\"] == \"Bachelor\"\n",
    "\n",
    "# Skill extraction test\n",
    "test_df = spark.createDataFrame([(\"Excellent writing; Foreign language\")], [\"Preferred Skills\"])\n",
    "test_df = extract_skills(test_df)\n",
    "skills = [row[\"Skill\"] for row in test_df.collect()]\n",
    "assert \"Excellent writing\" in skills and \"Foreign language\" in skills\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights\n",
    "- Most postings are concentrated in a few job categories.\n",
    "- Salary distributions vary widely across categories.\n",
    "- Higher degrees (Master/Doctorate) generally correlate with higher salaries.\n",
    "- Certain agencies consistently post the highest-paying jobs.\n",
    "- Skills like data analysis, project management, and specialized technical expertise are linked to higher salaries.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
